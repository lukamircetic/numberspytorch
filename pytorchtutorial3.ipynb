{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST('',train=True,download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST('',train=False,download=True,transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train,batch_size=10,shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__() ##output can be whatever you want but we go with 64\n",
    "        self.fc1 = nn.Linear(28*28,64) ##28*28 image = 784 which is input\n",
    "        self.fc2 = nn.Linear(64,64)    ##takes prevs output\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "        self.fc4 = nn.Linear(64,10) ##10 numbers so 10 possible outputs\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x,dim=1)\n",
    "        \n",
    "        \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(28,28)\n",
    "X = X.view(-1,28*28)  ##negative one means input will be unknown shape -1 and 1 gives exact same answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3713, -2.2266, -2.3930, -2.3513, -2.2762, -2.4057, -2.3939, -2.1757,\n",
       "         -2.1122, -2.3700]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1180, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0667, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2183, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#loss - measure of how wrong is a model - want this to decrease over time\n",
    "#optimizer - adjusts weights based on loss model\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.001) ##size of step of gradient\n",
    "\n",
    "\n",
    "EPOCHS = 3\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        #data is a bunch of feature sets and labels\n",
    "        X,y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1,28*28))\n",
    "        loss = F.nll_loss(output,y)\n",
    "        loss.backward() ##back propagation\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.983\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1,784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[3].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[3].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
